# Day 11
### 29 July, 2018

1. Decision Trees
  * Nodes: Split for the value of a certain attribute
  * Edges: Outcome of a split to next node
  * Root: The node that performs the first split
  * Leaves: Terminal nodes that predict the outcome
   ---
  * Entropy and Information Gain are the mathematical models of choosing the best split. (Deciding the Root split and the order of other following splits)

2. Random Forests
  * Different splits in the training data can lead to very different decision trees.
  * Bagging is a general purpose procedure for reducing the variance of machine learning methods.
  * Random forests are built on the idea of Bagging.
  * We create an ensemble of decision trees using bootstrap samples of the training set.
  * While building each tree, each time a split is considered, a random sample of M features is chosen as a split candidate from the full set of P features. The split is allowed to use only one of those M features.
  * A new random sample of features is chosen for every single tree at every single split.
  * For classification, M is typically chosen to be square root of P.
  * Watch the course video for understanding Random forests.

3. Steps:
  * Prepare the data
  * Train test split
  * Fit the model
  * Make predictions
  * Use confusion matrix and collection report to analyse predictions.

4. Random Forests will always perform better than Decision Trees and the difference would be more significant in bigger datasets.

5. Tree Visualization